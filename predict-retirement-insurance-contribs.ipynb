{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canadian Household Income Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plots\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants to make key column names easier to reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_HOUSEHOLDS_COL = \"HSBASHHD\"\n",
    "INSURANCE_COL = \"HSEP001S\"\n",
    "PORTION_RET_INSUR_COL = \"portion_retirement_insurance\" # Our target column that we will calculate\n",
    "INCOME_COL = \"HSHNIAGG\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = pl.read_csv(\"./data/HouseholdSpend.csv\")\n",
    "ds = pl.read_csv(\"./data/DemoStats.csv\")\n",
    "\n",
    "# Check for initial null or NaN values\n",
    "# Assert no NaNs in a DataFrame with float columns\n",
    "assert not hs.drop_nans().shape[0] < hs.shape[0], \"Spending DataFrame contains NaN values\"\n",
    "assert not ds.drop_nans().shape[0] < ds.shape[0], \"Demographics DataFrame contains NaN values\"\n",
    "assert not hs.drop_nulls().shape[0] < hs.shape[0], \"Spending DataFrame contains null values\"\n",
    "assert not ds.drop_nulls().shape[0] < ds.shape[0], \"Demographics DataFrame contains null values\"\n",
    "\n",
    "display(hs.describe())\n",
    "display(ds.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the zip code and geography level columns - these are just IDs that won't help our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.drop_in_place(\"CODE\")\n",
    "hs.drop_in_place(\"GEO\")\n",
    "ds.drop_in_place(\"GEO\")\n",
    "ds.drop_in_place(\"CODE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently all the household spending variables represent total spending for entire neighbourhood; we will convert them to average per household spending which is probably a better predictor of per household insurance contribution. We don't want to confuse the model with some neighbourhoods having dramatically higher spending only because of high population.\n",
    "\n",
    "Then we will construct our target variable, the portion of income a household spends on insurance and retirement savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_households = hs.select(TOTAL_HOUSEHOLDS_COL)\n",
    "hs.drop_in_place(TOTAL_HOUSEHOLDS_COL)\n",
    "hs = hs.with_columns(\n",
    "    pl.all() / total_households[TOTAL_HOUSEHOLDS_COL]\n",
    ")\n",
    "\n",
    "# Construct our target variable: portion of income spent on insurance and retirement\n",
    "hs = hs.with_columns(\n",
    "    (hs[INSURANCE_COL] / hs[INCOME_COL]).alias(PORTION_RET_INSUR_COL)\n",
    ")\n",
    "\n",
    "hs = hs.fill_nan(0) # replace NaNs resulting from zip codes with no households; we will remove these later\n",
    "hs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some redundant features. We'll first drop features highly correlated with our target variable. Then, we'll read in the metadata file for the dataset, which is structured like a pre-order depth first search tree traversal. We'll use this metadata to identify and delete the non-\"leaf node\" features that don't give us any new info. For example, \"Total Population\" is redundant when we have \"Total Male Population\", \"Total Female Population\" and \"Total Other Population\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns used to calculate target or highly correlated with the target (related to income or insurance spend)\n",
    "columns_to_drop = [INCOME_COL, \"HSAGDISPIN\", \"HSAGDISCIN\", INSURANCE_COL]\n",
    "for col in columns_to_drop:\n",
    "    hs.drop_in_place(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_drop(metadata_file):\n",
    "    # Read the metadata file\n",
    "    df = pl.read_csv(metadata_file)\n",
    "    \n",
    "    #Initialize lists for variables to drop\n",
    "    non_leaf_variables = []\n",
    "    summary_variables = []\n",
    "    \n",
    "    # process rows in order to identify non-leaf nodes\n",
    "    for i in range(len(df)):\n",
    "        var = df['Variable'][i]\n",
    "        hier_level = df['Hierarchy Level'][i]\n",
    "        \n",
    "\n",
    "        # Check if this is a summary variable (ends with AVG or MED)\n",
    "        if var.endswith('AVG') or var.endswith('MED'):\n",
    "            summary_variables.append(var)\n",
    "            continue\n",
    "        \n",
    "        # Check if the next row exists and is a direct child\n",
    "        if i < len(df) - 1 and df['Hierarchy Level'][i+1] == hier_level + 1:\n",
    "            # This is a parent node (has at least one child)\n",
    "            non_leaf_variables.append(var)\n",
    "    \n",
    "    #  Combine all var to drop\n",
    "    columns_to_drop = list(set(non_leaf_variables +  summary_variables))\n",
    "    \n",
    "    return columns_to_drop\n",
    "\n",
    "\n",
    "ds_columns_to_drop = get_columns_to_drop(\"data\\DemoStats 2024 - Metadata.csv\")\n",
    "hs_columns_to_drop = get_columns_to_drop(\"data\\HouseholdSpend 2024 - Metadata.csv\")\n",
    "\n",
    "# set aside insurance and retirement - this is our target variable\n",
    "\n",
    "dropped_hs_columns = []\n",
    "for col in hs_columns_to_drop:\n",
    "    if col in hs.columns:\n",
    "        hs.drop_in_place(col)\n",
    "        dropped_hs_columns.append(col)\n",
    "print(f\"Dropped columns from hs: {', '.join(dropped_hs_columns)}\")\n",
    "\n",
    "dropped_ds_columns = []\n",
    "for col in ds_columns_to_drop:\n",
    "    if col in ds.columns:\n",
    "        ds.drop_in_place(col)\n",
    "        dropped_ds_columns.append(col)\n",
    "print(f\"Dropped columns from ds: {', '.join(dropped_ds_columns)}\")\n",
    "\n",
    "\n",
    "display(hs.describe())\n",
    "display(ds.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the two datasets together so we have a single feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.concat([hs, ds], how=\"horizontal\")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows where total population is 0 or > 5% of the values are negative/invalid. These represent empty postal codes that will only add noise to our data.\n",
    "Also drop columns where all values are zero or > 5% of the values are negative/invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where total population is 0 or has too many invalid values\n",
    "invalid_row_threshold = 0.05  # 5% threshold\n",
    "\n",
    "# filter rows where > 5% of values are invalid\n",
    "data = data.filter(\n",
    "    ~(pl.sum_horizontal(pl.all().lt(0)) / pl.sum_horizontal(pl.all().is_not_null()) > invalid_row_threshold)\n",
    ")\n",
    "\n",
    "# filter out rows where total population is 0\n",
    "data = data.filter(~pl.all_horizontal(pl.all().eq(0)))\n",
    "\n",
    "# filter out columns where > 5% of values are invalid\n",
    "numeric_cols = [s.name for s in data]\n",
    "cols_to_drop = [col for col in numeric_cols if (data[col] < 0).sum() / hs.height > invalid_row_threshold]\n",
    "data.drop(cols_to_drop)\n",
    "\n",
    "# drop all zero columns\n",
    "data.drop(\n",
    "    [col for col in hs.columns if (data[col] == 0).all()]\n",
    ")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the spending variables have negative values which is obviously not valid. We will impute them with the median. We can see from the display() output that some columns' max spending values are in the 9-figure range. This is way outside a typical neighbourhood's spend in any category. We will cap extreme outliers by truncating all values above 3 * IQR for all the columns in the spending dataframe.\n",
    "\n",
    "Finally, we will standardize the data using z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeValueImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.medians = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # X can be a DataFrame or np  array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.medians = {}\n",
    "            for col in X.columns:\n",
    "                # Calculate median of non-negative values\n",
    "                non_negative_values = X.loc[X[col] >= 0, col]\n",
    "                self.medians[col] = non_negative_values.median() if len(non_negative_values) > 0 else 0\n",
    "        else:\n",
    "            self.medians = []\n",
    "            for i in range(X.shape[1]):\n",
    "                non_negative_values = X[:, i][X[:, i] >= 0]\n",
    "                self.medians.append(np.median(non_negative_values) if len(non_negative_values) > 0 else 0)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        if isinstance(X_copy, pd.DataFrame):\n",
    "            for col in X_copy.columns:\n",
    "                mask = X_copy[col] < 0\n",
    "                X_copy.loc[mask, col] = self.medians[col]\n",
    "        else:  # numpy array\n",
    "            for i in range(X_copy.shape[1]):\n",
    "                mask = X_copy[:, i] < 0\n",
    "                X_copy[mask, i] = self.medians[i]\n",
    "                \n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class IQRClippingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, multiplier= 3):\n",
    "        self.multiplier = multiplier\n",
    "        self.upper_bounds_ = {  }   \n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = pl.DataFrame(X)\n",
    "          \n",
    "        for col in df.columns:\n",
    "            q1 = df[col].quantile(.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            self.upper_bounds_[col] = q3 + self.multiplier * iqr\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = pl.DataFrame(X)    \n",
    "         \n",
    "        for col in df.columns:  \n",
    "            if col in self.upper_bounds_:\n",
    "                upper_bound = self.upper_bounds_[ col]\n",
    "                df = df.with_columns(\n",
    "                     pl.when(pl.col(col) > upper_bound)\n",
    "                    .then(upper_bound)\n",
    "                    .otherwise(pl.col(col)) \n",
    "                    .alias(col) )\n",
    "        \n",
    "        return  df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[PORTION_RET_INSUR_COL]\n",
    "X = data.drop(PORTION_RET_INSUR_COL)\n",
    "X = X.with_row_index(\"index\")\n",
    "\n",
    "# Impute negative values and clip outliers\n",
    "# Construct the pipeline\n",
    "preprocess = Pipeline([\n",
    "    ('impute_negatives', NegativeValueImputer()),\n",
    "    ('clip_outliers', IQRClippingTransformer(multiplier=3)),\n",
    "    ('standardize', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the data\n",
    "# X_processed = preprocess.fit_transform(X.to_pandas())\n",
    "\n",
    "# Convert back to Polars DataFrame\n",
    "# X_processed = pl.DataFrame(X_processed)\n",
    "\n",
    "# set columns\n",
    "# X_processed.columns = X.columns\n",
    "\n",
    "# display(X_processed.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We will undersample the feature matrix to improve clustering runtime. Then we'll preprocess the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample to reduce training time\n",
    "X_proc_kmeans_sampled = X.sample(fraction=0.1, seed=SEED) \n",
    "X_kmeans_sampled = X.sample(fraction=0.1, seed=SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to the data\n",
    "X_proc_kmeans_sampled = preprocess.fit_transform(X_proc_kmeans_sampled.to_pandas())\n",
    "\n",
    "# Convert back to Polars DataFrame\n",
    "X_proc_kmeans_sampled = pl.DataFrame(X_proc_kmeans_sampled)\n",
    "\n",
    "# set columns\n",
    "X_proc_kmeans_sampled.columns = X.columns\n",
    "\n",
    "display(X_proc_kmeans_sampled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method\n",
    "Let's try to use the elbow method to identify the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(random_state=SEED)\n",
    "vis = KElbowVisualizer(kmean, k=(2, 12))\n",
    "vis.fit(X_proc_kmeans_sampled.to_pandas())  \n",
    "optimal_k = vis.elbow_value_\n",
    "final_kmeans = KMeans(n_clusters=optimal_k, random_state=2025)\n",
    "cluster_labels = final_kmeans.fit_predict(X_proc_kmeans_sampled.to_pandas())\n",
    "vis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_res = X_kmeans_sampled.group_by(pl.Series(cluster_labels)).mean()\n",
    "clust_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow lies at k=4, suggesting 4 clusters is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_models = []\n",
    "labels = []\n",
    "for clusts in range(2, 12):\n",
    "    agg_models.append(KMeans(n_clusters=clusts))\n",
    "    labels.append(f'KMeans Clustering, Clusters = {clusts}')\n",
    "\n",
    "plots.plot_silhouettes(agg_models, labels, X_proc_kmeans_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly, the silhouette plot agrees with the elbow method in that choosing fewer clusters is better, with k=4 (the best value identified by the cluster method) having a better average silhouette score compared to higher k values. However, no number of clusters produces a particularly good clustering. The silhouette plots show that regardless of the number of clusters used, most clusters have negative silhouette scores, suggesting most points are closer to other clusters than their own. This could stem from multiple factors related to the dataset. In high-dimensional spaces, distances tend to become less meaningful due to the curse of dimensionality, making it difficult for clustering algorithms to identify well-separated groups. k-means in particular assumes convex clusters and may struggle with complex structures. Or perhaps the Euclidean distance used is not suitable for this dataset, as is often the case when there are many features. To improve clustering performance, techniques such as PCA for dimensionality reduction, alternative clustering methods (such as spectral clustering methods), and different distance metrics (e.g., cosine similarity) could be considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
