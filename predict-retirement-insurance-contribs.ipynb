{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canadian Household Income Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plots\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some constants to make key column names easier to reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_HOUSEHOLDS_COL = \"HSBASHHD\"\n",
    "INSURANCE_COL = \"HSEP001S\"\n",
    "PORTION_RET_INSUR_COL = \"portion_retirement_insurance\" # Our target column that we will calculate\n",
    "INCOME_COL = \"HSHNIAGG\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = pl.read_csv(\"./data/HouseholdSpend.csv\")\n",
    "ds = pl.read_csv(\"./data/DemoStats.csv\")\n",
    "\n",
    "# Check for initial null or NaN values\n",
    "# Assert no NaNs in a DataFrame with float columns\n",
    "assert not hs.drop_nans().shape[0] < hs.shape[0], \"Spending DataFrame contains NaN values\"\n",
    "assert not ds.drop_nans().shape[0] < ds.shape[0], \"Demographics DataFrame contains NaN values\"\n",
    "assert not hs.drop_nulls().shape[0] < hs.shape[0], \"Spending DataFrame contains null values\"\n",
    "assert not ds.drop_nulls().shape[0] < ds.shape[0], \"Demographics DataFrame contains null values\"\n",
    "\n",
    "display(hs.describe())\n",
    "display(ds.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the zip code and geography level columns - these are just IDs that won't help our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.drop_in_place(\"CODE\")\n",
    "hs.drop_in_place(\"GEO\")\n",
    "ds.drop_in_place(\"GEO\")\n",
    "ds.drop_in_place(\"CODE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently all the household spending variables represent total spending for entire neighbourhood; we will convert them to average per household spending which is probably a better predictor of per household insurance contribution. We don't want to confuse the model with some neighbourhoods having dramatically higher spending only because of high population.\n",
    "\n",
    "Then we will construct our target variable, the portion of income a household spends on insurance and retirement savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_households = hs.select(TOTAL_HOUSEHOLDS_COL)\n",
    "hs.drop_in_place(TOTAL_HOUSEHOLDS_COL)\n",
    "hs = hs.with_columns(\n",
    "    pl.all() / total_households[TOTAL_HOUSEHOLDS_COL]\n",
    ")\n",
    "\n",
    "# Construct our target variable: portion of income spent on insurance and retirement\n",
    "hs = hs.with_columns(\n",
    "    (hs[INSURANCE_COL] / hs[INCOME_COL]).alias(PORTION_RET_INSUR_COL)\n",
    ")\n",
    "\n",
    "hs = hs.fill_nan(0) # replace NaNs resulting from zip codes with no households; we will remove these later\n",
    "hs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some redundant features. We'll first drop features highly correlated with our target variable. Then, we'll read in the metadata file for the dataset, which is structured like a pre-order depth first search tree traversal. We'll use this metadata to identify and delete the non-\"leaf node\" features that don't give us any new info. For example, \"Total Population\" is redundant when we have \"Total Male Population\", \"Total Female Population\" and \"Total Other Population\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns used to calculate target or highly correlated with the target (related to income or insurance spend)\n",
    "columns_to_drop = [INCOME_COL, \"HSAGDISPIN\", \"HSAGDISCIN\", INSURANCE_COL]\n",
    "for col in columns_to_drop:\n",
    "    hs.drop_in_place(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_drop(metadata_file):\n",
    "    # Read the metadata file\n",
    "    df = pl.read_csv(metadata_file)\n",
    "    \n",
    "    #Initialize lists for variables to drop\n",
    "    non_leaf_variables = []\n",
    "    summary_variables = []\n",
    "    \n",
    "    # process rows in order to identify non-leaf nodes\n",
    "    for i in range(len(df)):\n",
    "        var = df['Variable'][i]\n",
    "        hier_level = df['Hierarchy Level'][i]\n",
    "        \n",
    "\n",
    "        # Check if this is a summary variable (ends with AVG or MED)\n",
    "        if var.endswith('AVG') or var.endswith('MED'):\n",
    "            summary_variables.append(var)\n",
    "            continue\n",
    "        \n",
    "        # Check if the next row exists and is a direct child\n",
    "        if i < len(df) - 1 and df['Hierarchy Level'][i+1] == hier_level + 1:\n",
    "            # This is a parent node (has at least one child)\n",
    "            non_leaf_variables.append(var)\n",
    "    \n",
    "    #  Combine all var to drop\n",
    "    columns_to_drop = list(set(non_leaf_variables +  summary_variables))\n",
    "    \n",
    "    return columns_to_drop\n",
    "\n",
    "\n",
    "ds_columns_to_drop = get_columns_to_drop(\"data\\DemoStats 2024 - Metadata.csv\")\n",
    "hs_columns_to_drop = get_columns_to_drop(\"data\\HouseholdSpend 2024 - Metadata.csv\")\n",
    "\n",
    "# set aside insurance and retirement - this is our target variable\n",
    "\n",
    "dropped_hs_columns = []\n",
    "for col in hs_columns_to_drop:\n",
    "    if col in hs.columns:\n",
    "        hs.drop_in_place(col)\n",
    "        dropped_hs_columns.append(col)\n",
    "print(f\"Dropped columns from hs: {', '.join(dropped_hs_columns)}\")\n",
    "\n",
    "dropped_ds_columns = []\n",
    "for col in ds_columns_to_drop:\n",
    "    if col in ds.columns:\n",
    "        ds.drop_in_place(col)\n",
    "        dropped_ds_columns.append(col)\n",
    "print(f\"Dropped columns from ds: {', '.join(dropped_ds_columns)}\")\n",
    "\n",
    "\n",
    "display(hs.describe())\n",
    "display(ds.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the two datasets together so we have a single feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.concat([hs, ds], how=\"horizontal\")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop rows where total population is 0 or > 5% of the values are negative/invalid. These represent empty postal codes that will only add noise to our data.\n",
    "Also drop columns where all values are zero or > 5% of the values are negative/invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where total population is 0 or has too many invalid values\n",
    "invalid_row_threshold = 0.05  # 5% threshold\n",
    "\n",
    "# filter rows where > 5% of values are invalid\n",
    "data = data.filter(\n",
    "    ~(pl.sum_horizontal(pl.all().lt(0)) / pl.sum_horizontal(pl.all().is_not_null()) > invalid_row_threshold)\n",
    ")\n",
    "\n",
    "# filter out rows where total population is 0\n",
    "data = data.filter(~pl.all_horizontal(pl.all().eq(0)))\n",
    "\n",
    "# filter out columns where > 5% of values are invalid\n",
    "numeric_cols = [s.name for s in data]\n",
    "cols_to_drop = [col for col in numeric_cols if (data[col] < 0).sum() / hs.height > invalid_row_threshold]\n",
    "data.drop(cols_to_drop)\n",
    "\n",
    "# drop all zero columns\n",
    "data.drop(\n",
    "    [col for col in hs.columns if (data[col] == 0).all()]\n",
    ")\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the spending variables have negative values which is obviously not valid. We will impute them with the median. We can see from the display() output that some columns' max spending values are in the 9-figure range. This is way outside a typical neighbourhood's spend in any category. We will cap extreme outliers by truncating all values above 3 * IQR for all the columns in the spending dataframe.\n",
    "\n",
    "Finally, we will standardize the data using z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeValueImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.medians = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # X can be a DataFrame or np  array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.medians = {}\n",
    "            for col in X.columns:\n",
    "                # Calculate median of non-negative values\n",
    "                non_negative_values = X.loc[X[col] >= 0, col]\n",
    "                self.medians[col] = non_negative_values.median() if len(non_negative_values) > 0 else 0\n",
    "        else:\n",
    "            self.medians = []\n",
    "            for i in range(X.shape[1]):\n",
    "                non_negative_values = X[:, i][X[:, i] >= 0]\n",
    "                self.medians.append(np.median(non_negative_values) if len(non_negative_values) > 0 else 0)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        if isinstance(X_copy, pd.DataFrame):\n",
    "            for col in X_copy.columns:\n",
    "                mask = X_copy[col] < 0\n",
    "                X_copy.loc[mask, col] = self.medians[col]\n",
    "        else:  # numpy array\n",
    "            for i in range(X_copy.shape[1]):\n",
    "                mask = X_copy[:, i] < 0\n",
    "                X_copy[mask, i] = self.medians[i]\n",
    "                \n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class IQRClippingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, multiplier= 3):\n",
    "        self.multiplier = multiplier\n",
    "        self.upper_bounds_ = {  }   \n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = pl.DataFrame(X)\n",
    "          \n",
    "        for col in df.columns:\n",
    "            q1 = df[col].quantile(.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            self.upper_bounds_[col] = q3 + self.multiplier * iqr\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = pl.DataFrame(X)    \n",
    "         \n",
    "        for col in df.columns:  \n",
    "            if col in self.upper_bounds_:\n",
    "                upper_bound = self.upper_bounds_[ col]\n",
    "                df = df.with_columns(\n",
    "                     pl.when(pl.col(col) > upper_bound)\n",
    "                    .then(upper_bound)\n",
    "                    .otherwise(pl.col(col)) \n",
    "                    .alias(col) )\n",
    "        \n",
    "        return  df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[PORTION_RET_INSUR_COL]\n",
    "X = data.drop(PORTION_RET_INSUR_COL)\n",
    "\n",
    "display(X)\n",
    "display(y)\n",
    "\n",
    "# Impute negative values and clip outliers\n",
    "# Construct the pipeline\n",
    "preprocess = Pipeline([\n",
    "    ('impute_negatives', NegativeValueImputer()),\n",
    "    ('clip_outliers', IQRClippingTransformer(multiplier=3)),\n",
    "    ('standardize', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the data\n",
    "# X_processed = preprocess.fit_transform(X.to_pandas())\n",
    "\n",
    "# Convert back to Polars DataFrame\n",
    "# X_processed = pl.DataFrame(X_processed)\n",
    "\n",
    "# set columns\n",
    "# X_processed.columns = X.columns\n",
    "\n",
    "# display(X_processed.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We will undersample the feature matrix to improve clustering runtime. Then we'll preprocess the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample to reduce training time\n",
    "X_proc_kmeans_sampled = X.sample(fraction=0.1, seed=SEED) \n",
    "X_kmeans_sampled = X.sample(fraction=0.1, seed=SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to the data\n",
    "X_proc_kmeans_sampled = preprocess.fit_transform(X_proc_kmeans_sampled.to_pandas())\n",
    "\n",
    "# Convert back to Polars DataFrame\n",
    "X_proc_kmeans_sampled = pl.DataFrame(X_proc_kmeans_sampled)\n",
    "\n",
    "# set columns\n",
    "X_proc_kmeans_sampled.columns = X.columns\n",
    "\n",
    "display(X_proc_kmeans_sampled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method\n",
    "Let's try to use the elbow method to identify the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(random_state=SEED, n_init=10)\n",
    "vis = KElbowVisualizer(kmean, k=(2, 12))\n",
    "vis.fit(X_proc_kmeans_sampled.to_pandas())  \n",
    "optimal_k = vis.elbow_value_\n",
    "final_kmeans = KMeans(n_clusters=optimal_k, random_state=SEED, n_init=10)\n",
    "elb_cluster_labels = final_kmeans.fit_predict(X_proc_kmeans_sampled.to_pandas())\n",
    "vis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elb_clust_res = X_kmeans_sampled.group_by(pl.Series(elb_cluster_labels)).mean()\n",
    "elb_clust_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow lies at k=6, suggesting 6 clusters is ideal. The clusters are difficult to interpret due to the high dimensionality. Generally, the ratio between the average value of each cluster for each column is the same, signalling that overall spending explains the most variance between the clusters. In other words, rich people (cluster 4) spend more on just about everything compared to the poorest (cluster 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_models = []\n",
    "labels = []\n",
    "for clusts in range(2, 12):\n",
    "    sil_models.append(KMeans(n_clusters=clusts, random_state=SEED, n_init=10))\n",
    "    labels.append(f'KMeans Clustering, Clusters = {clusts}')\n",
    "plots.plot_silhouettes(sil_models, labels, X_proc_kmeans_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_cluster_labels = sil_models[0].fit_predict(X_proc_kmeans_sampled.to_pandas())\n",
    "sil_clust_res = X_kmeans_sampled.group_by(pl.Series(sil_cluster_labels)).mean()\n",
    "sil_clust_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly, the silhouette plot agrees with the elbow method in that choosing fewer clusters is better, with k=2 (the best value identified by the cluster method) having a better average silhouette score (0.38) compared to higher k values. However, no number of clusters produces a particularly good clustering. k=2 produces two imbalanced clusters with the cluster labelled 0 having significantly fewer points and worse average silhouette score. Higher k values are even worse, with most points  closer to other clusters than their own. This could stem from multiple factors related to the dataset. In high-dimensional spaces, distances tend to become less meaningful due to the curse of dimensionality, making it difficult for clustering algorithms to identify well-separated groups. k-means in particular assumes convex clusters and may struggle with complex structures. Or perhaps the Euclidean distance used is not suitable for this dataset, as is often the case when there are many features. To improve clustering performance, techniques such as PCA for dimensionality reduction, alternative clustering methods (such as spectral clustering methods), and different distance metrics (e.g., cosine similarity) could be considered.\n",
    "\n",
    "Similar to the k = 6 clustering, the k = 2 clustering seems to have simply divided the dataset into a slightly higher income group that spends a bit more on everything (cluster 0) vs. a slightly lower income group (cluster 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod_param_grid = {\n",
    "    'elasticnet__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    'elasticnet__l1_ratio': [0.0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "full_pipeline_lin = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('elasticnet', ElasticNet(max_iter=10000, random_state=SEED))\n",
    "])\n",
    "\n",
    "# sample for quick test\n",
    "X = X.sample(fraction=0.1, seed=SEED)\n",
    "y = y.sample(fraction=0.1, seed=SEED)\n",
    "\n",
    "X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=SEED\n",
    ")\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "lin_mod_grid_search = GridSearchCV(\n",
    "    full_pipeline_lin,\n",
    "    param_grid=lin_mod_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lin_mod_grid_search.fit(X_train_lin.to_pandas(), y_train_lin.to_pandas())\n",
    "print(\"Best parameters found: \", lin_mod_grid_search.best_params_)\n",
    "best_model = lin_mod_grid_search.best_estimator_\n",
    "y_pred_lin = best_model.predict(X_test_lin.to_pandas())\n",
    "\n",
    "# Plot scatterplot of predicted vs actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_lin, y_pred_lin, alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs Actual Proportion of Income Spent on Insurance and Retirement')\n",
    "plt.plot([y_test_lin.min(), y_test_lin.max()], [y_test_lin.min(), y_test_lin.max()], 'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_CI_regression(y_test, y_pred, n_bs=1000, alpha= 0.05):\n",
    "    \"\"\"Calculate bootstrap confidence intervals for MSE and R2 scores in regression\n",
    "    \n",
    "    Args:\n",
    "        y_test (array-like): True target values\n",
    "        y_pred (array-like): Predicted target values\n",
    "        n_bs (int, optional): Number of bootstrap iterations.  Defaults to 1000.\n",
    "        alpha (float, optional): Significance level for confidence interval. Defaults to 0.05.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Two tuples containing:\n",
    "    - (mse_ci_lower,  mse_ci_upper): Lower and upper bounds ofMSE confidence interval\n",
    "    - (r2_ci_lower, r2_ci_upper): Lower and upper bounds of R sq confidence interval\n",
    "    \"\"\"\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test,  y_pred)\n",
    "    \n",
    "    bs_mse =  np.zeros(n_bs)\n",
    "    bs_r2 =  np.zeros(n_bs)\n",
    "    \n",
    "    for i in range(n_bs):\n",
    "        rand_idx = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "        bs_mse[i] = mean_squared_error(y_test[rand_idx], y_pred[rand_idx])\n",
    "        bs_r2[i] = r2_score(y_test[rand_idx], y_pred[rand_idx])\n",
    "    \n",
    "    bs_mse_spread = bs_mse -  mse\n",
    "    bs_r2_spread = bs_r2 - r2\n",
    "     \n",
    "     \n",
    "    mse_ci_lower = mse - np.percentile(bs_mse_spread, 100 * (1 - alpha / 2))\n",
    "    mse_ci_upper = mse - np.percentile(bs_mse_spread, 100 * (alpha / 2))\n",
    "    r2_ci_lower = r2 - np.percentile(bs_r2_spread, 100 * (1 - alpha / 2))\n",
    "    r2_ci_upper = r2 - np.percentile(bs_r2_spread, 100 * (alpha / 2))\n",
    "    \n",
    "    return (mse_ci_lower, mse_ci_upper), (r2_ci_lower, r2_ci_upper)\n",
    "\n",
    "mse_ci, r2_ci = bootstrap_CI_regression(y_test_lin.to_pandas(), y_pred_lin)\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test_lin.to_pandas(), y_pred_lin):.4}\")\n",
    "print(f\"R-squared: {r2_score(y_test_lin.to_pandas(), y_pred_lin):.4}\")\n",
    "print(f\"Bootstrap 95% CI for MSE: ({mse_ci[0]:.4}, {mse_ci[1]:.4})\")\n",
    "\n",
    "print(f\"Bootstrap 95% for Rsquared: ({r2_ci[0]:.4}, {r2_ci[1]:.4})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 5 most impactful features and their ranking\n",
    "feature_importances = best_model.named_steps['elasticnet'].coef_\n",
    "feature_names = X.columns\n",
    "sorted_indices = np.argsort(np.abs(feature_importances))[::-1][:5]\n",
    "print(\"Top 5 most impactful features and their coefficients:\")\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{feature_names[idx]}: {feature_importances[idx]:.9f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
